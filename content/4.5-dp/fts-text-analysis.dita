<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_o23_j34_1v">
  <title>Text Analysis</title>
  <body>
    <p>Text Analysis is the process of transforming input text into a series of terms, or canonical forms of words that are typically stripped of inflections and junk characters. Analysis is done on the text documents when you create the index, and again on the text of your query when you perform a search. </p>
    <p>When people talk about full text search being “language aware”, they are usually referring to the results of text analysis.</p>
    <p><b>Example</b></p>
    <p>Let’s imagine we have a document containing the text, “watery light beer”. And let’s imagine a user searches for “watered down”.</p>
    <p>There is no exact match in this case, but if we want this query to match this document we can accomplish this by choosing the right analyzer. In this case, we want to use an analyzer that can stem English words. </p>
    <p>By doing this, the index entry for the document will contain the terms: “water”, “light”, “beer”. And the query will search for terms: “water”, “down”.</p>
    <p>Now we see that the terms will match and the user will get search results.</p>
    <p>Stemming is just one way in which the text can be transformed, the key here is that choosing the right analyzer is essential to getting high quality search results.</p>
    <section> <title>Analyzers</title>
      <p>Analyzers are used to transform input text into a stream of tokens for indexing. Full Text Search includes a number of analyzers or you can build your own. Analyzers are built up from modular components.<ul>
        <li>Character Filters strip out undesirable characters from the input. For example, the “html” character filter ignores html tags and just indexes the content from html documents.</li>
        <li>Tokenizers split input strings into a token stream. Typically you’re trying to create a token for each word.</li>
        <li>Token Filters are chained together to perform additional post processing on the token stream.</li></ul>  </p> </section>
    <section><title>Tokenizers</title>
      <p>Tokenizers cut sequences of characters into meaningful units (tokens) while ignoring useless characters, like spaces or commas.</p>
      <p>Stemming is the process of reducing words to an underlying base form. The goal of stemming is to get rid of grammatical inflections while preserving the words’ meanings. Those underlying base forms are then written into the index.</p>
      <p>In the Couchbase Server Web Console, you can find a list of available stemmers on the Index > Full Text > Edit or Create New Index > Analyzers. If you click “New Analyzer” you see a list of available stemmers in the “Token Filters” dropdown.</p>
      <p><b>Single Token</b> </p> 
      <p>The Single Token Tokenizer will return the entire input bytes as a single token. This can be useful for things like URLs (http://www.example.com/blog/stuff+and+nonsense/) or email address (will.spammington@couchbase.com) that might otherwise be broken up at punctuation or special character boundaries. It could also be used when multi-word phrases should be indexed as a single term, for example, in the case of proper place names that would otherwise be broken into multiple terms by the Whitespace tokenizer. </p>
      <p>You might use a Single Token analyzer for any of the fields included in the excerpt from the berr-sample. <codeblock>{
        "type": "brewery",
        "name": "21st Amendment Brewery Cafe",
        "city": "San Francisco",
        "country": "United States",
        "phone": "1-415-369-0900",
        "website": "http://www.21st-amendment.com/",
        ...
        }</codeblock> </p>
      <p><b>Regular Expression</b> </p> 
      <p>The Regular Expression Tokenizer will tokenize input using a configurable regular expression. The regular expression should match token text. See the Whitespace Tokenizer for an example of its usage.
      </p>
      <p><b>Whitespace</b> </p> 
      <p>The Whitespace Tokenizer is an instance of the Regular Expression Tokenizer. It uses the regular expression \w+. For many western languages this may work well as a simple tokenizer.</p>
      <p><b>Unicode</b> </p> 
      <p>The Unicode Tokenizer uses the <xref href="https://github.com/blevesearch/segment"
          format="html" scope="external">segment</xref> library to perform <xref
          href="http://www.unicode.org/reports/tr29/" format="html" scope="external">Unicode Text
          Segmentation</xref> on word boundaries. It is recommended over the ICU tokenizer for all
        languages not requiring dictionary-based tokenization that is supported by ICU.</p>
      <p>Full Text Search includes a number of analyzers or you can build your own. </p> </section>
    <section><title>Customizing the Field Mappings</title>
      <p><b>Dynamic Index Mapping</b> </p> 
      <p>When the indexer encounters a field whose type you haven’t explicitly specified, it guesses
        the type by looking at the JSON value. <table frame="all" rowsep="1" colsep="1"
          id="table_dcs_gl4_1v">
          <tgroup cols="2" align="left">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Type of JSON value</entry>
                <entry>Indexed as...</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Boolean</entry>
                <entry>Boolean</entry>
              </row>
              <row>
                <entry>Number</entry>
                <entry>Number</entry>
              </row>
              <row>
                <entry>String containing a date</entry>
                <entry>Date</entry>
              </row>
              <row>
                <entry>String (not containing a date)</entry>
                <entry>String</entry>
              </row>
            </tbody>
          </tgroup>
        </table> The indexer attempts to parse String values as dates and indexes them as such if the operation succeeds. Note that other String values like “7” or “true” will never be indexed as numbers or booleans. The number type is modeled as 64-bit floating point value internally.</p>    </section>
  </body>
</topic>
