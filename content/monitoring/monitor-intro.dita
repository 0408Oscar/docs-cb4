<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic xml:lang="en-us" id="topic227">
  <title>Monitoring</title>
  <shortdesc>There are a number of different ways to monitor Couchbase Server, including underlying
    processes, ports, and queuing.</shortdesc>

<body>
<section><title>Underlying server process</title>
  <p>These processes occur whether or not the server is handling reads/writes actively or
    handling other operations from a client application. Right after you start up a node, you may
    notice a spike in CPU utilization, and the utilization rate will plateau at some level greater
    than zero. The following describes the ongoing processes that are running on your node:</p>
  <ul>
    <li><codeph>beam.smp</codeph> on Linux; <codeph>erl.exe</codeph> on Windows</li>
  </ul>
  <p>These processes are responsible for monitoring and managing all other underlying server
    processes such as ongoing XDCR replications, cluster operations, and views.</p>
  <p>There is a separate monitoring/babysitting process running on each node. The process is small
    and simple and, therefore, unlikely to crash due to lack of memory. It is responsible for
    spawning and monitoring the second, larger process for cluster management, XDCR, and views. It
    also spawns and monitors the processes for Moxi and memcached. If any of these three processes
    fail, the monitoring process will re-spawn them.</p>
  <p>The main benefit of this approach is that an Erlang VM crash will not cause the
    <codeph>Moxi</codeph> and <codeph>memcached</codeph> processes to crash. You will see the 
    <codeph>beam.smp</codeph> or <codeph>erl.exe</codeph> processes running on Linux or Windows.</p>
  <p>Information about the processes mentioned in this section is written to the
    log file <codeph>ns_server.babysitter.log</codeph>, which can be gathered with the
    <codeph>cbcollect_info</codeph> utility. </p>
  <ul>
    <li><codeph>memcached</codeph>: This process is responsible for caching items in RAM and persisting
      them to disk.</li>
    <li><codeph>moxi</codeph>: This process enables third-party memcached clients to connect to the
      server.</li>
  </ul>


</section>  
  <section><title>Stats and Port 11210</title>Any communication (stats or data) to a port other than
        <codeph>11210</codeph> will result in the request going through a Moxi process. <p>In the
        Couchbase cluster, all stats requests will be aggregated across the cluster and might
        produce some inconsistencies or confusion when looking at stats that cannot be  aggregated.
        </p><p>In general, it is best to run all your stat commands against port
          <codeph>11210</codeph>, which will always give you the information for the specific node
        that you are sending the request to. It is a best practice to aggregate the relevant data
        across nodes at a higher level in your script or monitoring system.</p><p>When you run the
        commands, and all stats commands without supplying a bucket name and password, they will
        return results for the default bucket and produce an error if one does not exist.</p><p>To
        access a bucket other than the default, you will need to supply the bucket name and password
        at the end of the command. Any bucket created on a dedicated port does not require a
        password. </p><p>The TCP/IP port allocation on Windows by default includes a restricted
        number of ports available for client communication. </p></section>   
  <section>
      <title>Disk write queue </title>
      <p>Couchbase Server is a persistent database, which means that monitoring of the system
        requires an understanding of how we interact with the disk subsystem. Since Couchbase Server
        is an asynchronous system, any mutation operation is committed first to DRAM and then queued
        to be written to disk. An acknowledgment is returned to the client almost immediately so
        that it can continue working. </p>
      <p>Disk writing is implemented as a 2-queue system: comitted to DRAM and then queued to be
        written to disk.</p>
      <p>The first queue is where mutations are immediately placed. Whenever there are items in that
        queue, our “flusher” (disk writer) comes along and takes all the items off of that queue,
        places them into the other one and begins writing to disk. Since disk performance is so
        dramatically different from RAM, new writes can be continuously accepted while the system is
        (possibly slowly) writing new ones to the disk.</p>
      <p>The flusher will process 250k items at a time, and then perform a disk commit and continue
        this cycle until its queue is drained. When it has completed everything in its queue, it
        will either grab the next group from the first queue or sleep until there are more items to
        write.</p>
    </section>  
    
 
  
</body>
  <related-links>
    <linklist>
      <link href="http://msdn.microsoft.com/en-us/library/aa560610(v=bts.20).aspx" scope="external"
        format="html">
        <linktext>MSDN: Avoiding TCP/IP Port Exhaustion</linktext></link>
      <link href="../CLI/cbcollect_info_tool.dita"></link>
    </linklist>
  </related-links>

</topic>
