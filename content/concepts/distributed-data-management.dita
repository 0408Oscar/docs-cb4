<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_cz5_szf_vs">
 <title>Distributed data management </title>
 <shortdesc>Couchbase Server is a distributed system that is built from the ground up for easy scale out and management.</shortdesc>
 <conbody>
  <p> It is typically deployed on a cluster of commodity servers although for development purposes all functionality can be run on a single node. This also means that an application developed at small scale (even on a laptop) can be deployed to a distributed cluster, or separate clusters of varied topologies, without any architecture or behavioral changes to that application.  Couchbase Server has a peer-to-peer topology and all the nodes play the same role in a cluster, that is, all the nodes are equal and communicate to each other on demand. There is no concept of master nodes, slave nodes, config nodes, name nodes, head nodes, etc.  This model works particularly well with cloud infrastructure, in which nodes can be added or removed without considering their “type”. Capacity can be increased or decreased simply by adding or removing nodes. In this way a cluster can grow CPU, RAM, disk and network capacity by adding physical servers or virtual machines that have the exact same software installed. Nodes may be easily be added or removed via a rebalance process, which re-distributes the data evenly across all nodes. The rebalance process is done online and requires no application downtime, and can be initiated at the click of a button.</p>
<section> <title>Architecture of a Single Node</title>
 <p>
  A Couchbase Server cluster consists of a group of interchangeable, largely self-sufficient nodes. There is just one Couchbase Server node type; every Couchbase node consists of the same components, services, and interfaces. Having a single node type greatly simplifies the installation, configuration, management and troubleshooting of a Couchbase Server cluster, both in terms of what human operators must do and in terms of what the automatic management needs to do.
  </p>
 <p>The main components of a Couchbase Server node are the cluster manager, the data service, the query service, the index service, and the underlying managed cache and storage components. By dividing up potentially conflicting workloads in this way, a Couchbase Server node can achieve maximum throughput and resource utilization and minimum latency.</p>
 <p>
  The <b>Cluster Manager</b> handles the cluster level operations and the coordination between nodes in a Couchbase Server cluster. These management functions include configuring nodes, monitoring nodes, gathering statistics, logging, and controlling data rebalancing among cluster nodes. The cluster manager determines the node’s membership in a cluster, responds to heartbeat requests, monitors its own services and repairs itself if possible.  </p>
  
  <p>Although each node runs its own local Cluster Manager, there is only one node chosen from among them, called the orchestrator, that supervises the cluster at a given point in time. The orchestrator maintains the authoritative copy of the cluster configuration, and performs the necessary node management functions to avoid any conflicts from multiple nodes interacting. If any node becomes unresponsive for any reason, the orchestrator notifies the other nodes in the cluster and promotes the relevant replicas to active status. This process is called failover, and it can be done automatically or manually.  If the orchestrator fails or loses communication with the cluster for any reason, the remaining nodes detect the failure when they stop receiving the its heartbeat so they immediately elect a new orchestrator.  This is done immediately and transparently to the operations of the cluster.
 </p>
 <p>The <b>Data Service</b> handles core data management operations, such as Key Value GET/SET. The data service is also responsible for building and maintaining map-reduce views. Note that even though map-reduce views can act as indexes, they are always managed by the data service, not the indexing service.</p>
  
  <p>The Data Manager performs data storage and retrieval. It consists of a managed cache (based on memcached), a storage engine, and the view query engine.</p>
<p>
 The <b>Indexing Service</b> efficiently maintains indexes for fast query execution. This involves the creation and deletion of indexes, keeping those indexes up to date based on change notifications from the data service and responding to index-scan requests from the query service.  The Indexing Service is accessed and managed through the Query Service.
</p>
 <p>
  The <b>Query Service</b> handles N1QL query parsing, optimization, and execution. This service interacts with both the indexing service and the data service to process and return those queries back to the requesting application.  
 </p>
 <p>
  The <b>Storage Layer</b> persists data from RAM to disk and reads data back into memory when needed. Couchbase Server’s storage engines are responsible for persisting data to disk. They write every change operation that a node receives to an append only file (AOF), meaning that mutations go to the end of the file and in-place updates are forbidden. This file is replayed if necessary at node startup to repopulate the locally managed cache.</p>
  
  <p>Couchbase Server periodically compacts its data files and index files to save space (auto-compaction). During compaction, each node remains online and continues to process read and write requests.  Couchbase Server performs compaction in parallel, both across the many nodes of a cluster as well as across the many files in persistent storage. This highly parallel processing allows very large datasets to be compacted incrementally. 
  
 </p>
 <p>
  Each data node relies on a built-in multi-threaded <b>Managed Cache</b> that is used both for the data service and separately for the indexing service.  These caches are actively managed as opposed to relying upon the operating system’s file buffer cache and/or memory-mapped files.</p>
  
  <p>The data service’s integral managed cache is an object-managed cache based on memcached.  All key-value operations are performed via the cache.  Applications can access Couchbase using memcached compatible APIs such as get, set, delete, append, and prepend. Because this API is complete and transparent to the application, Couchbase Server can be used as a drop in replacement for memcached. </p>
  
  <p>There are many challenges with memcached that Couchbase Server overcomes. Couchbase Server handles clustering, online rebalancing, and auto-sharding of data across nodes, so that memory and other resources can be elastically increased or reduced as needed to support the workload. Because Couchbase Server automatically replicates data, cache availability is greatly improved and end users rarely hit a cold cache even if system components fail. Finally, Couchbase Server presents a built-in admin console for monitoring and managing the entire cluster.</p>
  
  <p>The managed caching layer within the indexing service is built into the underlying storage engine (ForestDB in 4.0).  It manages the caching of individual portions of indexes as RAM is available to speed performance both of inserting/updating information as well as servicing index scans.
  
 </p>
<p><b>Buckets and vBuckets</b></p>
 <p>
  
  Within Couchbase, a bucket is a logical collection of related documents, like a database in an RDBMS. It is a unique key space.  Couchbase administrators and developers work with buckets when performing tasks such as accessing and managing data, checking statistics, and issuing N1QL queries. A bucket should not be viewed as a “table”, and very often contains documents of varying schemas.  Buckets are used to segregate the configuration and operational handling of data in terms of cache allocation, indexing and replication.  While buckets can play a role in the concept of multi-tenancy, they are not necessarily the only component.  For further information, please read here.</p>
  
  <p>Internally, Couchbase Server uses a mechanism called vBuckets (synonymous to shard or partition) to automatically distribute data across nodes, a process sometimes known as “auto-sharding”. vBuckets help enable data replication, failover, and dynamic cluster reconfiguration. Unlike buckets, users do not manipulate vBuckets directly. Couchbase Server divides each bucket into 1024 vBuckets and distributes them evenly across the nodes running the data service within a cluster.  vBuckets do not have a fixed physical location on nodes; therefore there is a mapping of vBuckets to nodes known as the cluster map. Through the Couchbase client libraries, the application automatically and transparently distributes the data and workload across these vBuckets (see below). More on vBuckets can be found here. 
 </p>
 <p><b>Data Change Protocol</b></p>
 <p>
  
  Data Change Protocol (DCP) is a high-performance streaming protocol that communicates the state of the data using an ordered change log with sequence numbers.  It is a generic protocol designed for keeping an external consumer of the data service up to date. </p>
  
  <p>DCP is robust and resilient in the face of transitory errors. For example, if a stream is interrupted, DCP resumes from exactly the point of its last successful update once connectivity resumes. DCP provides version histories and rollbacks that provide a snapshot of the data. Snapshots are used to create a brand new replica or catch up a partially built but interrupted node. </p>
  
  <p>Couchbase Server leverages DCP internally and externally for several different purposes. Within Couchbase Server, DCP is used to create replicas of data within and between clusters, and to create and maintain indexes (both local and global).  DCP also feeds most  Couchbase Connectors, which are integrations with external systems such as Hadoop, Kafka or ElasticSearch. Some of these connectors leverage DCP directly or connect to Couchbase Server’s XDCR functionality (see below) to keep the external systems in sync by acting like Couchbase Server nodes.
 </p>
</section> 
  <section> <title>Replication</title>
   <p>
    Within a single cluster, Couchbase Server employs peer-to-peer replication. It automatically creates copies of active data, distributes those replicas across the nodes in the cluster, ensuring that every copy is located on a separate node, and then continues to maintain the replicas over time.
    Couchbase supports up to 3 replicas (= 4 copies of data). Peer-to-peer replication allows for the best performance and resource utilization as well as eliminating possible bottlenecks and single points of failure.  Each node replicates separate slices (vbuckets) of its active data to multiple other nodes so that the no one node is a replica for any other one node.</p>
    
    <p>If a node goes down, Couchbase Server recovers that data  by activating the replicas that already exist elsewhere in the cluster. This  process is known as failover. Failover can be automatic or manual, and failover can be scripted to satisfy specialized requirements.</p>
    
    <p>The redundancy that replication provides protects against the loss of data on any single node and helps increase data availability by allowing recovery from hardware failures and service interruptions. Replication also further increases read availability by servicing requests in the short time that an active copy is unavailable before failover takes place. </p>
    
    <p>As mentioned earlier, by default replicas are only for the purpose of high availability and are not used in the normal serving of data.  This allows Couchbase to be strongly consistent and applications immediately read their own writes by not ever requesting data from a node that it is not active on.  The managed caching layer of the data service allows for extremely high throughput and low latency with mixed workloads to either a small or large number of documents on a given node.  It is not uncommon to measure response times of under 1ms at the 99th percentile of 100’s of thousands of requests per second to a single node.</p>
    
    <p>Replicating data to different data centers using XDCR (see below) provides locality and increased availability of data for applications.
   </p>
  </section>
  <section> <title>Client topology awareness</title>
<p>   All Couchbase clients are automatically aware of the topology of the Couchbase Server cluster that they are connected to and transparently maintain this awareness for all 3 services (data, index, query) across any topology change or failure.  An application will typically be configured with the IP addresses of more than one Couchbase Server node for high availability when bootstrapping into the cluster.</p>
 
 <p>When interacting with the data service, Couchbase clients directly access any document in a Couchbase Server cluster, without consulting a proxy or external routing entity. This works because they store and transparently update a local copy of the complete cluster map based on topology change notifications from the Couchbase Server. In order to locate the active copy of any stored object, a Couchbase client uses a CRC32 hashing algorithm on the key and the list of vBuckets (1024) to identify which vBucket is responsible for that key. The client then consults its internal cluster map to determine which node is currently active for that vbucket. The access request is then sent directly to the node itself, without needing to lookup the location of that item in any external authority.  This is a faster, more efficient, more robust, and more scalable approach than routing through sharding proxies or performing meta-data lookups each operation.</p> 
 
 <p>Similar topology awareness is used to maintain a list of nodes running the N1QL query service, and that service in turn uses the same underlying cluster map when servicing  queries. The query service itself is stateless, so any query node can process any N1QL request. 
</p></section>
  <section> <title>Concurrency</title>
 <p>  In a busy distributed data management system, conflicts might arise with multiple clients attempting to write the same document simultaneously. All Couchbase Server operations are atomic at the document level.  Couchbase Server offers both optimistic and pessimistic locking to prevents concurrency errors. </p>
  
  <p>Optimistic locking is based on the CAS Value, a unique and atomically incrementing identifier that is part of every document’s metadata. Several update operations require a successful CAS value check in order to succeed. The application passes the CAS value as a parameter. Couchbase Server then verifies the CAS Value has not changed before a document is deleted or modified to effectively prevent conflicts without having to lock records.   This is the preferred, best practice, method of handling concurrency with Couchbase.</p>
  
  <p>Couchbase Server also supports pessimistic locking (similar to semaphores), which is less commonly used. Pessimistic locks are automatically released to prevent unrecoverable deadlocks.
 </p> </section>
  <section> <title>Multidimensional scaling</title>
   <p>Couchbase Server 4.0 and greater supports multidimensional scaling (MDS). MDS enables users to turn on or off specific services on each Couchbase Server node so that the node in effect becomes specialized to handle a specific workload: document storage (data service), global indexes (index service) or N1QL query processing (query service). </p>
   
   <p>MDS has three main advantages: 
   Each service can be independently scaled to suit an application’s evolution, whether that entails a growing data set, expanding indexing requirements, or increased query processing needs. 
   The index and query services work most quickly and efficiently when a single or small number of machines contain the entire index. 
   Users can elect to tailor machines to their workloads, for example, by adding more CPUs to a query node.  </p>
   
   <p>MDS allows specific services to both scale up and scale out without sacrificing ease of administration because they are all managed from within the same cluster and configured at runtime, and the software installed on each machine is identical.
   
   Figure 4 Multidimensional Scaling optionally disables services to dedicate nodes to certain workloads
   </p>
  </section>
  
 </conbody>
</concept>
