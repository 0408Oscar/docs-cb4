<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_ofb_4d5_xv">
  <title>SDK Opertions</title><shortdesc>The Java SDK offers methods you can use to accomplish the basic CRUD operations of creating, retrieving, updating, and deleting documents.</shortdesc>
  <body>
    <p>This section describes the properties and types of <codeph>Document</codeph> objects and how to use
      them.</p>
   	<section>
			<title>The Document</title>

			<p>The <codeph>Document</codeph> class encapsulates the consolidated representation of all
				attributes that relate to a document stored on a Couchbase Server cluster. It includes
				the document's identifier and related metadata. A <codeph>Document</codeph> object
				contains the following properties:</p>

			<table>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Name</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								<codeph>id</codeph>
							</entry>
							<entry>The (per bucket) unique identifier of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>content</codeph>
							</entry>
							<entry>The actual content of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>cas</codeph>
							</entry>
							<entry>The CAS (Compare And Swap) value of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>expiry</codeph>
							</entry>
							<entry>The expiration time of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>mutationToken</codeph>
							</entry>
							<entry>The optional MutationToken after a mutation.</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<p>There are a few different implementations of a <codeph>Document</codeph>, the most
				prominent one is the <codeph>JsonDocument</codeph>.</p>

			<p>Because Couchbase Server can store anything and not just JSON, many document types exist to
				satisfy the general needs of an application. You can also write your own
					<codeph>Document</codeph> implementations, which is not covered in this
				introduction.</p>

			<p>
				<note>Every <codeph>Document</codeph> has an associated <codeph>Transcoder</codeph> that handles
					serialization and deserialization to and from the target wire format. This
					conversion is transparent but needs to be taken into account when custom
					documents are implemented.</note>
			</p>

			<p>The following <codeph>Document</codeph> types are supported out of the box:</p>

			<p>
				<b>Documents with JSON content:</b>
			</p>

			<table>
				<tgroup cols="4">
					<thead>
						<row>
							<entry>Document Name</entry>
							<entry>Description</entry>
							<entry>Compatible: 2.x SDKs</entry>
							<entry>Compatible: 1.x Java SDK</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								<xref href="#documents-basics/json"><codeph>JsonDocument</codeph></xref>
							</entry>
							<entry>The default, which has a <codeph>JsonObject</codeph> at the top level
								content.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonarray"><codeph>JsonArrayDocument</codeph></xref>
							</entry>
							<entry>Similar to <codeph>JsonDocument</codeph>, but has a
									<codeph>JsonArray</codeph> at the top level content.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonBooleanDocument</codeph></xref>
							</entry>
							<entry>Stores JSON-compatible Boolean values.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonLongDocument</codeph></xref>
							</entry>
							<entry>Stores JSON compatible long (number) values.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonDoubleDocument</codeph></xref>
							</entry>
							<entry>Stores JSON compatible double (number) values.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonStringDocument</codeph></xref>
							</entry>
							<entry>Stores JSON compatible String values. Input is automatically wrapped
								with <codeph>"..."</codeph>.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonraw"><codeph>RawJsonDocument</codeph></xref>
							</entry>
							<entry>Stores any JSON value and should be used if custom JSON serializers such
								as Jackson or GSON are already in use.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<codeph>EntityDocument</codeph>
							</entry>
							<entry>Used with the Repository implementation to write and read POJOs into JSON and back.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<p>
				<b>Documents with other content:</b>
			</p>

			<table>
				<tgroup cols="4">
					<thead>
						<row>
							<entry>Document Name</entry>
							<entry>Description</entry>
							<entry>Compatible: 2.x SDKs</entry>
							<entry>Compatible: 1.x Java SDK</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								<xref href="#documents-basics/binary"><codeph>BinaryDocument</codeph></xref>
							</entry>
							<entry>Can be used to store arbitrary binary data.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/serializable"><codeph>SerializableDocument</codeph></xref>
							</entry>
							<entry>Stores objects that implement <codeph>Serializable</codeph> through
								default Java object serialization.</entry>
							<entry>No</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/legacy"><codeph>LegacyDocument</codeph></xref>
							</entry>
							<entry>Uses the <codeph>Transcoder</codeph> from the 1.x SDKs and can be used
								for full cross-compatibility between the old and new versions.</entry>
							<entry>No</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/string"><codeph>StringDocument</codeph></xref>
							</entry>
							<entry>Can be used to store arbitrary strings. They will not be quoted, but
								stored as-is and flagged as "String".</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<p>
				<note type="other" othertype="Backward compatibility">Other than the
						<codeph>LegacyDocument</codeph> class, which strives for full backward
					compatibility, all <codeph>Document</codeph> types are trying best effort in that
					regard. Specific constraints are noted in each document description, but for all
					types keep in mind that compression is not supported other than on the
						<codeph>LegacyDocument</codeph>.</note>
			</p>

		</section>

		<section>
			<title>CAS and Expiry</title>

			<p>Every <codeph>Document</codeph> also contains the <codeph>expiry</codeph> and
					<codeph>cas</codeph> properties. They are considered meta information and are
				optional. An expiration time of <codeph>0</codeph> means that no expiration is set at
				all, and a <codeph>CAS</codeph> value 0 means it won't be used.</p>

			<p>You can set the <codeph>expiry</codeph> to control when the document should be
				deleted:</p>

			<codeblock outputclass="language-java"><![CDATA[// Expire in 10 seconds.
JsonDocument.create("id", 10, content);]]></codeblock>

			<codeblock outputclass="language-java"><![CDATA[// Expire in 1 day.
JsonDocument.create("id", TimeUnit.DAYS.toSeconds(1), content);]]></codeblock>

			<p>The expiration time starts when the document has been successfully stored on the server,
				not when the document was created on the application server. Any expiration time larger
				than 30 days in seconds is considered absolute (as in a Unix time stamp), anything
				smaller is considered relative in seconds.</p>

			<p>The <codeph>CAS</codeph> value can either be set by you directly or is populated by the
				SDK when the <codeph>Document</codeph> is loaded from the server (which is the
				recommended way to use it).</p>

			<p>For detailed information about how to utilize CAS for optimistic concurrency control, see
					<xref href="#topic_ofb_4d5_xv/updatingdocuments" format="dita"/> .</p>
		</section>

		<section id="json">
			<title>JsonDocument</title>

			<p>Couchbase Server uses the JSON format as a first-class citizen. It is used for querying
				(via both views and N1QL) and represents the main storage format that should be
				used.</p>

			<p>The <codeph>JsonDocument</codeph> class has factory methods named
					<codeph>create()</codeph> that you use to create documents. If you do not want to
				pass in an expiration time or CAS value (just the ID and content) you do it like
				this:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonDocument doc = JsonDocument.create("id", content);]]></codeblock>

			<p>The content needs to be of type <codeph>JsonObject</codeph>, which ships with the Java SDK. It
				works very much like a <codeph>Map</codeph> object but makes sure only data types
				understood by JSON are used.</p>

			<p>An empty JSON document can be created like this:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty();]]></codeblock>

			<p>After it is created, you can use the various <codeph>put()</codeph> methods to insert
				data:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonArray friends = JsonArray.empty()
	.add(JsonObject.empty().put("name", "Mike Ehrmantraut"))
	.add(JsonObject.empty().put("name", "Jesse Pinkman"));

JsonObject content = JsonObject.empty()
	.put("firstname", "Walter")
	.put("lastname", "White")
	.put("age", 52)
	.put("aliases", JsonArray.from("Walt Jackson", "Mr. Mayhew", "David Lynn"))
	.put("friends", friends);]]></codeblock>

			<p>This generates a JSON document like this (unordered, because the actual content is
				stored in a <codeph>Map</codeph>):</p>

			<codeblock outputclass="language-json"><![CDATA[{
   "firstname":"Walter",
   "aliases":[
	  "Walt Jackson",
	  "Mr. Mayhew",
	  "David Lynn"
   ],
   "age":52,
   "friends":[
	  {
		 "name":"Mike Ehrmantraut"
	  },
	  {
		 "name":"Jesse Pinkman"
	  }
   ],
   "lastname":"White"
}]]></codeblock>

			<p>
				<image href="images/document-jsonobject.png" id="image_i31_yqb_1" width="650px"/>
			</p>

			<p>In addition, the <codeph>JsonObject</codeph> and <codeph>JsonArray</codeph> classes
				provide convenience methods to generate and modify them.</p>

			<p>The <codeph>JsonDocument</codeph> can then be passed into the various operations on the
					<codeph>Bucket</codeph>:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonDocument walter = JsonDocument.create("user:walter", content);
JsonDocument inserted = bucket.insert(walter);]]></codeblock>

			<p>If you want to read values out of the <codeph>JsonDocument</codeph>, you can use either
				the typed or untyped getter methods.</p>

			<codeblock outputclass="language-java"><![CDATA[int age = content.getInt("age");
String name = content.getString("firstname") + content.getString("lastname");]]></codeblock>

			<p>
				<note>If you are accessing values that potentially do not exist, you need to use boxed
					values (<codeph>Integer</codeph>, <codeph>Long</codeph>, <codeph>Boolean</codeph>)
					instead of their unboxed variants (<codeph>int</codeph>, <codeph>long</codeph>,
						<codeph>boolean</codeph>) to avoid getting <codeph>NullPointerException</codeph>
					exceptions. If you use unboxed variants, make sure to catch them properly.</note>
			</p>

		</section>

		<section id="jsonarray">
			<title>JsonArrayDocument</title>

			<p>The <codeph>JsonArrayDocument</codeph> class works exactly like the
					<codeph>JsonDocument</codeph> class, with the main difference that you can have a
				JSON array at the top level content (instead of an object).</p>

			<p>So if you create a <codeph>JsonArrayDocument</codeph> like this:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonArray content = JsonArray.from("Hello", "World", 1234);
bucket.upsert(JsonArrayDocument.create("docWithArray", content));]]></codeblock>

			<p>It will look like this on the server:</p>

			<p>
				<image href="images/document-jsonarray.png" id="image_json-array" width="650px" />
			</p>

			<p>If you want to read the <codeph>JsonArrayDocument</codeph> back, you need to tell the SDK that
				you explicitly want to deviate from the default. Do it for every document type other
				than <codeph>JsonDocument</codeph>:</p>

			<codeblock outputclass="language-java"><![CDATA[bucket.get("docWithArray", JsonArrayDocument.class);]]></codeblock>

		</section>

		<section id="jsonraw">
			<title>RawJsonDocument</title>

			<p>The <codeph>JsonObject</codeph> and <codeph>JsonArray</codeph> types have been added for
				developer convenience. In a lot of places, custom JSON handling is already in place
				through libraries like Jackson or Google GSON.</p>

			<p>Of course, we want to provide the best of both worlds, and this is where the
					<codeph>RawJsonDocument</codeph> comes into play. You can store and read the
				already stringified JSON, but the SDK properly marks it as JSON, so it is
				cross-compatible with all other documents.</p>

			<p>Here is how you can read and write raw JSON data. For clarity, a plain string is used
				but it is up to you to wire this up with Jackson or a similar JSON processor:</p>

			<codeblock outputclass="language-java"><![CDATA[// write the raw data
String content = "{\"hello\": \"couchbase\", \"active\": true}";
bucket.upsert(RawJsonDocument.create("rawJsonDoc", content));

// read the raw data
// prints RawJsonDocument{id='rawJsonDoc', cas=..., expiry=0, content={"hello": "couchbase", "active": true}}
System.out.println(bucket.get("rawJsonDoc", RawJsonDocument.class));

// read it parsed
// prints true
System.out.println(bucket.get("rawJsonDoc").content().getBoolean("active"));]]></codeblock>

			<p>
				<image href="images/document-rawjson.png" id="image_i31_yqb_3p" width="650px"/>
			</p>

			<p>
				<note>If you use the <codeph>RawJsonDocument</codeph> type, the SDK does not perform any
					validation because the expectation is that a JSON-compatible library is used,
					and additional overhead will be avoided.</note>
			</p>
		</section>

		<section id="jsonvalue">
			<title>JSON value documents</title>

			<p>The JSON specification also allows you to store different values as content, and it also
				specifies how these values need to be encoded. Because the type system of Java is
				not as rich as it could be, different document types are provided to represent
				different values that can be stored. Because the encoding is clearly defined, these
				JSON values are also compatible with other 2.0 SDKs.</p>

			<p>A word on compatibility with the 1.X Java SDK: in a best-effort way the SDK tries to read
				properly flagged data from the old SDKs, but it stores it under the new format,
				which is not readable by the old SDKs anymore. So if you care about back-and-forth
				compatibility only read those values from the new SDK or use the
					<codeph>LegacyDocument</codeph> right away. Another option is to use strings
				only on the old SDK, and then working with it back and forth should be safe.</p>

			<p>Backward compatibility for JSON value documents works only if the actual content is not
				compressed.</p>

			<p>The following documents exist, which all work similarly except the content type that can
				be stored:</p>

			<ul>
				<li>
					<codeph>JsonBooleanDocument</codeph>
				</li>
				<li>
					<codeph>JsonLongDocument</codeph>
				</li>
				<li>
					<codeph>JsonDoubleDocument</codeph>
				</li>
				<li>
					<codeph>JsonStringDocument</codeph>
				</li>
			</ul>

			<p>They are all encoded and decoded based on their <xref href="http://json.org"
					format="html" scope="external">JSON specification</xref>.</p>
		</section>

		<section id="binary">
			<title>BinaryDocument</title>
			<p>The <codeph>BinaryDocument</codeph> can be used to store and read arbitrary bytes. It is
				the only default codec that directly exposes the underlying low-level Netty
					<codeph>ByteBuf</codeph> objects.</p>
			<p>
				<note type="important">Because the raw data is exposed, it is important to free it after it has
					been properly used. Not freeing it will result in increased garbage collection
					and memory leaks and should be avoided by all means. See <xref
						href="#topic_ofb_4d5_xv/binary-memory" format="dita"/>.</note>
			</p>
			<p>Because binary data is arbitrary anyway, it is backward compatible with the old SDK regarding
				flags so that it can be read and written back and forth. Make sure it is not
				compressed in the old SDK and that the same encoding and decoding process is used on
				the application side to avoid data corruption.</p>
			<p>Here is some demo code that shows how to write and read raw data. The example writes
				binary data, reads it back, and then frees the pooled resources:</p>
			<codeblock outputclass="language-java"><![CDATA[// Create buffer out of a string
ByteBuf toWrite = Unpooled.copiedBuffer("Hello World", CharsetUtil.UTF_8);

// Write it
bucket.upsert(BinaryDocument.create("binaryDoc", toWrite));

// Read it back
BinaryDocument read = bucket.get("binaryDoc", BinaryDocument.class);

// Print it
System.out.println(read.content().toString(CharsetUtil.UTF_8));

// Free the resources
ReferenceCountUtil.release(read.content());]]></codeblock>
		</section>

		<section id="binary-memory">
			<title>Correctly managing buffers</title>
			<p>
				<codeph>BinaryDocument</codeph> allows users to get the rawest form of data out of
				Couchbase. It  exposes Netty's <codeph>ByteBuf</codeph>, byte buffers that can have
				various characteristics (on- or off-heap, pooled or unpooled). In general, buffers
				created by the SDK are pooled and off heap. You can disable the pooling in the
					<codeph>CouchbaseEnvironment</codeph> if you absolutely need that. </p>
			<p>
				As a consequence, the memory associated with the ByteBuf must be a little bit more managed by the developer than usual in Java.
			</p>
			<p> Most notably, these byte buffers are reference counted, and you need to know three main
				methods associated to buffer management: <ul>
					<li><codeph>refCnt()</codeph> gives you the current reference count. When it
						hits 0, the buffer is released back to its original pool, and it cannot be
						used anymore.</li>
					<li><codeph>release()</codeph> will decrease the reference count by 1 (by
						default).</li>
					<li><codeph>retain()</codeph> is the inverse of release, allowing you to prepare
						for multiple consumptions by external methods that you know will each
						release the buffer. </li>
				</ul></p>
			<p>You can also use <codeph>ReferenceCountUtil.release(something)</codeph> if you don't want to
				check if <codeph>something</codeph> is actually a <codeph>ByteBuf</codeph> (will do
				nothing if it's not something that is <apiname>ReferenceCounted</apiname>). </p>
			<note type="important">
				The SDK bundles the Netty dependency into a different package so that it doesn't clash with a dependency to another version of Netty you may have. As such, you need to use the classes and packages provided by the SDK (<codeph>com.couchbase.client.deps.io.netty</codeph>) when interacting with the API. For example, the <codeph>ByteBuf</codeph> for the content of a <codeph>BinaryDocument</codeph> is a <codeph>com.couchbase.client.deps.io.netty.buffer.ByteBuf</codeph>.
			</note>

			<p><b>What happens if I don't release?</b></p>
			<p>Basically, you leak memory... </p>
			<p>Netty will by default inspect a small percentage of <codeph>ByteBuf</codeph>
				creations and usage to try and detect leaks (in which case it will output a log,
				look for the "LEAK" keyword). </p>
			<p>You can tune that to be more eagerly monitoring all buffers by calling
					<codeph>ResourceLeakDetector.setLevel(PARANOID)</codeph>. <note type="important"
					>Note that this incurs quite an overhead and should only be activated in tests.
					In production (prod), setting it to <codeph>ADVANCED</codeph> is not as heavy as
					paranoid and can be a good middle ground.</note>
			</p>
			<p><b>What happens if I release twice (or the SDK releases once more after I do)?</b></p>
			<p>Netty will throw an <codeph>IllegalReferenceCountException</codeph>. The buffer that has
				RefCnt = 0 cannot be interacted with anymore since it means it has been freed back
				into the pool. </p>
			<p><b>When must I release?</b></p>
			<p>When the SDK creates a <codeph>BinaryDocument</codeph> for you, basically GET-type operations. </p>
			<p>Mutative operations, on the other hand, will take care of the buffer you pass in for you, at
				the time the buffer is written on the wire. </p>
			<p><b>When must I usually retain?</b></p>
			<p>When you do a write, the buffer will usually be released by the SDK calling
					<codeph>release()</codeph>. But if you implement a kind of fallback behavior
				(for instance attempt to <codeph>insert()</codeph> a doc, catch
					<codeph>DocumentAlreadyExistException</codeph> and then fallback to an
					<codeph>update()</codeph> instead), that means the SDK would attempt to release
				twice, which won't work. </p>
			<p>In this case you can <codeph>retain()</codeph> the buffer before the first attempt, let the
				catch block do the extra release if something goes wrong. You have to manage the
				extra release if the first write succeeds, and think about catching other possible
				exceptions (here also an extra release is needed): </p>
<codeblock outputclass="language-java"><![CDATA[byteBuffer.retain(); //prepare for potential multi usage (+1 refCnt, refCnt = 2)
try {
   bucket.append(document);
   // refCnt = 2 on success
   byteBuffer.release(); //refCnt = 1
} catch (DocumentDoesNotExistException dneException) {
   // buffer is released on errors, refCnt = 1
   //second usage will also release, but we want to be at refCnt = 1 for the finally block
   byteBuffer.retain(); //refCnt = 2
   bucket.insert(document); //refCnt = 1
} // other uncaught errors will still cause refCnt to be released down to 1
finally {
   //we made sure that at this point refCnt = 1 in any case (success, caught exception, uncaught exception)
   byteBuffer.release(); //refCnt = 0, returned to the pool
}]]></codeblock>
		</section>

		<section>
			<title>SerializableDocument</title>

			<p>Any object that implements <codeph>Serializable</codeph> can be safely encoded and
				decoded using the built-in Java serialization mechanism. While it is very convenient, it
				can be slow in cases where the POJOs are very complex and deeply nested. It is backward
				compatible with the old SDK unless the data has been compressed previously.</p>

			<p>Here is an example that serializes a POJO, deserializes it later, and then prints one of
				its properties:</p>

			<codeblock outputclass="language-java"><![CDATA[import java.io.Serializable;

public class User implements Serializable {

	private final String username;

	public User(String username) {
		this.username = username;
	}

	public String getUsername() {
		return username;
	}

}]]></codeblock>

			<codeblock outputclass="language-java"><![CDATA[// Create the User and store it
bucket.upsert(SerializableDocument.create("user::michael",  new User("Michael")));

// Read it back
SerializableDocument found = bucket.get("user::michael", SerializableDocument.class);

// Print a property to verify
System.out.println(((User) found.content()).getUsername());]]></codeblock>

		</section>

		<section id="legacy">
			<title>LegacyDocument</title>

			<p>The <codeph>LegacyDocument</codeph> is intended to be 1:1 compatible (including compression)
				with the 1.x Java SDK. For better compatibility with the other 2.0 SDKs, we
				recommend to move to JSON type documents (and other compatible ones), but the
					<codeph>LegacyDocument</codeph> is very helpful during data migration and
				side-by-side usage.</p>

			<p>Because the old and new SDKs don't share artifacts or namespaces, they can be used at
				the same time. If you're using Maven, you can add both a 1.x SDK and a 2.x SDK as
				dependencies in the <filepath>pom.xml</filepath> file. For example:</p>

			<codeblock outputclass="language-xml"><![CDATA[<dependencies>
	<dependency>
		<groupId>com.couchbase.client</groupId>
		<artifactId>java-client</artifactId>
		<version>2.2.7</version>
	</dependency>
	<dependency>
		<groupId>com.couchbase.client</groupId>
		<artifactId>couchbase-client</artifactId>
		<version>1.4.11</version>
	</dependency>
</dependencies>]]></codeblock>

			<p>Here is a snippet that writes a value using the old SDK and reads it out with the new
				one:</p>

			<codeblock outputclass="language-java"><![CDATA[// Open bucket on the new SDK
Cluster cluster = CouchbaseCluster.create();
Bucket bucket = cluster.openBucket();

// Open bucket on the old SDK
CouchbaseClient client = new CouchbaseClient(
	Arrays.asList(URI.create("http://127.0.0.1:8091/pools")),
	"default",
	""
);

// Create document on old SDK
client.set("fromOld", "Hello from Old!").get();

// Create document on new SDK
bucket.upsert(LegacyDocument.create("fromNew", "Hello from New!"));

// Read old from new
System.out.println(bucket.get("fromOld", LegacyDocument.class));

// Read new from old
System.out.println(client.get("fromNew"));

// Shutdown old client
client.shutdown();

// Shutdown new client
cluster.disconnect();]]></codeblock>

			<p>This prints:</p>

			<codeblock><![CDATA[LegacyDocument{id='fromOld', cas=1097880624822, expiry=0, content=Hello from Old!}
Hello from New!]]></codeblock>

		</section>

		<section id="string">
			<title>StringDocument</title>

			<p>This document type provides an SDK 2.0 cross-compatible way to exchange strings. It should not
				be mistaken with the <codeph>JsonStringDocument</codeph> that automatically quotes
				it and also flags it as JSON. It is also backward compatible unless compression was
				used previously.</p>

			<p>If a <codeph>String</codeph> is stored through it, it is explicitly flagged as a
				non-JSON string. The usage is straightforward:</p>

			<codeblock outputclass="language-java"><![CDATA[// Create the document
bucket.upsert(StringDocument.create("stringDoc", "Hello World"));

// Prints:
// StringDocument{id='stringDoc', cas=1424054670330, expiry=0, content=Hello World}
System.out.println(bucket.get("stringDoc", StringDocument.class));]]></codeblock>

			<p>You can use the <codeph>cbc</codeph> command line tool to compare the flags and actual
				content compared to the <codeph>JsonStringDocument</codeph>:</p>

			<codeblock outputclass="language-java"><![CDATA[bucket.upsert(StringDocument.create("stringDoc", "Hello World"));]]></codeblock>

			<codeblock outputclass="language-bash"><![CDATA[└──╼ cbc cat stringDoc
stringDoc            CAS=0x55668b55f010000, Flags=0x4000000. Size=11
Hello World]]></codeblock>

			<codeblock outputclass="language-java"><![CDATA[bucket.upsert(JsonStringDocument.create("jsonStringDoc", "Hello World"));]]></codeblock>

			<codeblock outputclass="language-bash"><![CDATA[└──╼ cbc cat jsonStringDoc
jsonStringDoc        CAS=0x84d77eb55f010000, Flags=0x2000000. Size=13
"Hello World"]]></codeblock>

			<p>You can see that the JSON string got automatically quoted and also has different flags
				applied to it.</p>

		</section>
  
  	
  	
  	
  	
  	
  	
  	
  	
  	<section id="creatingdocuments"><title>Creating Documents</title>
  	
  	<p>	This section describes creating documents by using the <codeph>insert()</codeph> or <codeph>upsert()</codeph> methods.</p>
  		
  	
  			
  	<sectiondiv>
  			<p><b>Insert</b></p>
  				
  				<p>The <codeph>insert</codeph> method allows you to store a <codeph>Document</codeph> if it does
  					not already exist in the bucket. If it does exist, the synchronous API throws a
  					<codeph>DocumentAlreadyExistsException</codeph> (or the
  					<codeph>Observable</codeph> propagates it to <codeph>onErro</codeph> in the case
  					of the asynchronous API).</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
//this will throw DocumentAlreadyExistException:
JsonDocument inserted = bucket.insert(doc);]]></codeblock>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
Observable<JsonDocument> inserted = bucket.async().insert(doc);
inserted.subscribe(
	System.out::println,
	//this will be called with DocumentAlreadyExistException:
	Throwable::printStackTrace
);]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it is
  					respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is inserted because its type is
  					inferred from the method argument and the corresponding <codeph>Transcoder</codeph> is
  					used to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is different from the
  					<codeph>Document</codeph> passed in. The returned document references some
  					values like its <codeph>id</codeph> and <codeph>content</codeph> but also has the
  					<codeph>CAS</codeph> value set.</p>
  				
  	</sectiondiv>
  			
  		<sectiondiv>
  				<p><b>Upsert</b></p>
  				
  				<p>The <codeph>upsert</codeph> method works similar to <codeph>insert</codeph>, but it also
  					overrides an already stored <codeph>Document</codeph> (so there is no
  					<codeph>DocumentAlreadyExistsException</codeph> thrown by the synchronous API
  					nor propagated by the asynchronous API).</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
JsonDocument inserted = bucket.upsert(doc);]]></codeblock>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
Observable<JsonDocument> inserted = bucket.async().upsert(doc);
inserted.subscribe(
	System.out::println,
	//this won't be called:
	Throwable::printStackTrace
);]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it is
  					respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is upserted because its type is
  					inferred from the method argument and the corresponding <codeph>Transcoder</codeph> is
  					used to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is a different one compared to the
  					<codeph>Document</codeph> passed in. It references some values like its
  					<codeph>id</codeph> and <codeph>content</codeph> but also has the
  					<codeph>CAS</codeph> value set.</p></sectiondiv>
  			
  			
  		<sectiondiv>
  				<p><b>Durability Requirements</b></p>
  				
  				<p>If no durability requirements are set on the <codeph>insert</codeph> or
  					<codeph>upsert</codeph> methods, the operation will succeed when the server
  					acknowledges the document in its managed cache layer. While this is a performant
  					operation, there might be situations where you want to make sure that your document
  					has been persisted or replicated so that it survives power outages and other node
  					failures.</p>
  				
  				<p>Both methods provide overloads to supply such requirements:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[D insert(D document, PersistTo persistTo);
D insert(D document, ReplicateTo replicateTo);
D insert(D document, PersistTo persistTo, ReplicateTo replicateTo);

D upsert(D document, PersistTo persistTo);
D upsert(D document, ReplicateTo replicateTo);
D upsert(D document, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
  				
  				<note type="tip">The synchronous API also provides the same methods with custom timeouts, whereas the asynchronous API in <codeph>AsyncBucket</codeph> would rely on RxJava's <codeph>timeout</codeph> operator.</note>
  				
  				<p>You can configure either just one or both of the requirements when inserting or upserting. From an application point of view nothing needs to be changed when working with the response, although there is something that need to be kept in mind:</p>
  				
  				<p>The internal implementation first performs a regular <codeph>insert</codeph> or
  					<codeph>upsert</codeph> operation and afterward starts polling the specifically
  					affected cluster nodes for the state of the document. If something fails during this
  					operation (and failing the <codeph>Observable</codeph>), the original operation
  					might have succeeded nonetheless.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Insert the document and make sure it is persisted to the master node
bucket.insert(document, PersistTo.MASTER);

// Insert the document and make sure it is replicate to one replica node
bucket.insert(document, ReplicateTo.ONE);

// Insert the document and make sure it is persisted to one node and replicated to two
bucket.insert(document, PersistTo.ONE, ReplicateTo.TWO);]]></codeblock>
  				
  		</sectiondiv>
  			
  		<sectiondiv>
  				<p><b>Batching</b></p>
  				
  				<p>Because everything is asynchronous internally, batching <codeph>inserts</codeph> or
  					<codeph>upserts</codeph> can be achieved with the  <codeph>Observable</codeph>
  					functionality of the <codeph>AsyncBucket</codeph>.</p>
  				
  				<p>A combination of <codeph>just()</codeph> and <codeph>flatMap()</codeph> is used to store them without blocking:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument doc1 = JsonDocument.create("id1", content);
JsonDocument doc2 = JsonDocument.create("id2", content);
JsonDocument doc3 = JsonDocument.create("id3", content);

Observable
    .just(doc1, doc2, doc3)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(JsonDocument document) {
            return bucket.async().insert(document);
        }
    }).subscribe();]]></codeblock>
  				
  				<p>For the blocking API, batching is currently not supported. It is recommended to fall back on the asynchronous API for best performance.</p>
  				
  			
  		</sectiondiv>   
      
    </section>
   
  		
  	
  	
  	
  	
  	
  	
  	<section id="updatingdocuments"><title>Updating Documents</title>
  		
  	<p>This section describes updating documents by using the <codeph>replace()</codeph> or <codeph>upsert()</codeph> methods.</p>
  		
  		
  			
  			<sectiondiv>
  				<p>Replace</p>
  				
  				<p>The <codeph>replace</codeph> method replaces the <codeph>Document</codeph> if it exists, but fails with a <codeph>DocumentDoesNotExistException</codeph> otherwise.</p>
  				
  				<p>In addition, if the <codeph>CAS</codeph> value is set on the <codeph>Document</codeph> (not
  					equal to <codeph>0</codeph>), it is respected and passed to the server. If the
  					<codeph>CAS</codeph> value does not match the current server value, it fails
  					with a <codeph>CASMismatchException</codeph>.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
JsonDocument inserted = bucket.replace(doc)]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it will be respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is replaced: its type is inferred
  					from the method argument, and the corresponding <codeph>Transcoder</codeph> is used
  					to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is a different one compared to the
  					<codeph>Document</codeph> passed in. It references some values like its
  					<codeph>id</codeph> and <codeph>content</codeph> but also has the new
  					<codeph>CAS</codeph> value set.</p>
  				
  				<p>The following asynchronous sample will automatically take the <codeph>CAS</codeph> value into account because it is populated from the <codeph>get()</codeph> call and respected on the <codeph>replace()</codeph> call.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[bucket.async()
    .get("id")
    .map(new Func1<JsonDocument, JsonDocument>() {
        @Override
        public JsonDocument call(JsonDocument document) {
            modifyDocumentSomehow(document);
            return document;
        }
    })
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(JsonDocument document) {
            return bucket.async().replace(document);
        }
    }).subscribe();]]></codeblock>
  				
  				<p>Since this operation can fail if there is a <codeph>CASMismatchException</codeph>, a common pattern is to retry the complete process until it succeeds:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[Observable
    .defer(() -> bucket.async()get("id"))
    .map(document -> {
        document.content().put("modified", new Date().getTime());
        return document;
    })
    .flatMap(doc -> bucket.async().replace(doc))
    .retryWhen(attempts ->
        attempts.flatMap(n -> {
            if (!(n.getThrowable() instanceof CASMismatchException)) {
                return Observable.error(n.getThrowable());
            }
            return Observable.timer(1, TimeUnit.SECONDS);
        })
    )
    .subscribe();]]></codeblock>
  				<p>This code snippet uses <codeph>defer()</codeph>. This was necessary before SDK 2.2.0 to always do a fresh <codeph>get()</codeph>, but is now already done by the SDK.</p>
  				<note type="remember">Before version 2.2.0, a Subject was directly used in the SDK internally,
  					caching the value and, therefore, a resubscribe would just return the same value.
  					<codeph>defer()</codeph> makes sure to create a new one on every resubscribe. </note>
  				<p>Afterward, it artificially modifies the document and then tries to store it through a
  					<codeph>replace()</codeph> call. If it succeeds all is good, if it fails with an
  					<codeph>Exception</codeph> the <codeph>retryWhen()</codeph> block is executed.
  					In this block, the code checks if it is a <codeph>CASMismatchException</codeph> and
  					if so, executes a <codeph>timer</codeph> before retrying. Other errors could be
  					handled in there as well (even with different retry strategies), but in this
  					example, other errors are passed along.</p>
  				
  				<p>Note that since 2.1.2, such code is easier to write by use of the <codeph>RetryBuilder</codeph>:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[Observable
    .defer(() -> bucket.async().get("id"))
    .map(document -> {
        document.content().put("modified", new Date().getTime());
        return document;
    })
    .flatMap(doc -> bucket.async().replace(doc))
    .retryWhen(RetryBuilder
      .anyOf(CASMismatchException.class)
      .delay(Delay.fixed(1, TimeUnit.SECONDS))
      .once() //alternatively use max(n) to attempt n times total
      .build())
    .subscribe();]]></codeblock>
  				
  			</sectiondiv>
  			
  		<sectiondiv>
  			<p><b>Upsert</b></p>
  				
  				<p>The <codeph>upsert</codeph> method works similar to <codeph>replace</codeph>, but it also stores the <codeph>Document</codeph> if it does not exist (so there is no <codeph>DocumentDoesNotExistException</codeph> thrown).</p>
  				
  				<p>It also does not use the <codeph>CAS</codeph> value to handle concurrent updates, even when set on the document. Use <codeph>replace</codeph> instead.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
JsonDocument inserted = bucket.upsert(doc)]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it will be respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is upserted; its type is inferred
  					from the method argument, and the corresponding <codeph>Transcoder</codeph> is used
  					to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is a different one compared to the
  					<codeph>Document</codeph> passed in. It references some values like its
  					<codeph>id</codeph> and <codeph>content</codeph> but also has the
  					<codeph>CAS</codeph> value set.</p></sectiondiv>
  			
  			
  		<sectiondiv>
  				<p><b>Durability Requirements</b></p>
  				
  				<p>If no durability requirements are set on the <codeph>replace</codeph> or <codeph>upsert</codeph> methods, the operation will succeed when the server acknowledges the document in its managed cache layer. While this is a performant operation, there might be situations where you want to make sure that your document has been persisted and/or replicated so that it survives power outages and other node failures.</p>
  				
  				<p>Both methods provide overloads to supply such requirements:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[D replace(D document, PersistTo persistTo);
D replace(D document, ReplicateTo replicateTo);
D replace(D document, PersistTo persistTo, ReplicateTo replicateTo);

D upsert(D document, PersistTo persistTo);
D upsert(D document, ReplicateTo replicateTo);
D upsert(D document, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
  				
  				<p>You can configure either just one or both of the requirements when inserting or upserting. From an application point of view nothing needs to be changed when working with the response, although there is something that need to be kept in mind:</p>
  				
  				<p>The internal implementation first performs a regular <codeph>replace</codeph> or
  					<codeph>upsert</codeph> operation and afterward starts polling the specifically
  					affected cluster nodes for the state of the document. If something fails during this
  					operation (and failing the <codeph>Observable</codeph>), the original operation
  					might have succeeded nonetheless.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Update the document and make sure it is persisted to the master node
bucket.replace(document, PersistTo.MASTER);

// Update the document and make sure it is replicate to one replica node
bucket.replace(document, ReplicateTo.ONE);

// Update the document and make sure it is persisted to one node and replicated to two
bucket.replace(document, PersistTo.ONE, ReplicateTo.TWO);]]></codeblock>
  		</sectiondiv>
  			
  			
  		<sectiondiv>
  				<p>Batching</p>
  				
  				<p>Because everything is asynchronous internally, batching <codeph>replaces</codeph> or
  					<codeph>upserts</codeph> can be achieved with the <codeph>Observable</codeph>
  					functionality of the <codeph>AsyncBucket</codeph>.</p>
  				
  				<p>A combination of <codeph>just()</codeph> and <codeph>flatMap()</codeph> is used to store them without blocking:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument doc1 = JsonDocument.create("id1", content);
JsonDocument doc2 = JsonDocument.create("id2", content);
JsonDocument doc3 = JsonDocument.create("id3", content);

Observable
    .just(doc1, doc2, doc3)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(JsonDocument document) {
            return bucket.async().replace(document);
        }
    }).subscribe();]]></codeblock>
  				
  		</sectiondiv>		
      
      
    </section>
    
  	
  	<section><title>Retrieving Documents</title>
  		
  	<p>This section describes how to load documents using the various <codeph>get()</codeph>
  			methods.</p>
  	
  			
  		<sectiondiv>
  				<p><b>Regular reads</b></p>
  				
  				<p>To read a document use the <codeph>get()</codeph> method. Either pass in the
  					<codeph>id</codeph> of the <codeph>Document</codeph> or the
  					<codeph>Document</codeph> from which the <codeph>id</codeph> is taken from.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument loadedFromId = bucket.get("id");
JsonDocument loadedFromDoc = bucket.get(JsonDocument.create("id"));]]></codeblock>
  				
  				<p>Both methods have the same effect. The latter method is helpful if you are already
  					dealing with <codeph>Document</codeph> instances in your code and you don't want to
  					extract the <codeph>ID</codeph> out of them on your own.</p>
  				
  				<p>When only the ID is passed in, there is no way to figure out which
  					<codeph>Document</codeph> type should be used, so <codeph>JsonDocument</codeph> is
  					selected as a sensible default. If you want to override this, you can pass in a specific
  					<codeph>Document</codeph> type like this:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[LegacyDocument loaded = bucket.get("legacyId", LegacyDocument.class);]]></codeblock>
  				
  				<p>If the document is not found, the blocking API returns null.:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument found = bucket.get("notexisting");
if (found == null) {
// doc not found
} else {
// doc found
}]]></codeblock>
  				
  				<p>If you are dealing with asynchronous code, an empty <codeph>observable</codeph> is returned
  					instead. This aligns with how <codeph>Observable</codeph> objects are supposed to
  					work by contract, but also makes it easier to deal with the implementation later. If
  					no document is returned, the subsequent operations are just not executed, which
  					avoids having null checks all over the place (if for example a Document would be
  					returned but with the content set to <codeph>null</codeph>).</p>
  				<codeblock outputclass="language-java"><![CDATA[Observable<JsonDocument> updateIfFound = bucket.async().get(potentialNonExistingKey)
	.map(doc -> doc.content())
	.filter(jsonObject -> jsonObject.containsKey("test"))
	.subscribe(
		//onNext only invoked if the key could be retrieved
		data -> System.out.println("Data exists and has test key"),
		error::printStackTrace,
		() -> System.out.println("Completed"));]]></codeblock>
  				
  		</sectiondiv>
  			
  		<sectiondiv>
  				<p><b>Reading from replica</b></p>
  				
  				<p>A regular read always reads the document from its master node. If this node is down or
  					not available, the document cannot be loaded. Reading from replica allows you to load
  					the document from one or more replica nodes instead.</p>
  				
  				<p>
  					<note>When replica reads are used, always use them under the assumption that the data
  						returned is stale. There is no way to guarantee that the data is up-to-date on the
  						replica node unless the proper durability requirements have been set and succeeded on
  						write operations. Only use replica reads if you understand the implications.</note>
  				</p>
  				
  				<p>You can either read the data from one specific replica or all of the available
  					replicas:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Read from all available replicas and the master node and return all responding
bucket.getFromReplica("id", ReplicaMode.ALL);

// Read only from the first replica
bucket.getFromReplica("id", ReplicaMode.FIRST);

// Read only from the second replica
bucket.getFromReplica("id", ReplicaMode.SECOND);

// Read only from the third replica
bucket.getFromReplica("id", ReplicaMode.THIRD);]]></codeblock>
  				
  				<note type="note">If <codeph>ReplicaMode.ALL</codeph> is used, requests are sent to the master
  					node and all configured replicas.</note>
  				
  				<p>The main goal is to get responses back as fast as possible, but because more requests are
  					sent, more responses can arrive. You can use this to either compare all of the
  					responding documents and draw conclusions, or just pick the first one arriving:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[bucket
	.async()
    .getFromReplica("id", ReplicaMode.ALL)
    .first()
    .subscribe();]]></codeblock>
  				
  				<p>In addition, you can add operations to filter based on some of your assumptions. Imagine
  					you have a <codeph>version</codeph> field in your document and you want to only use the
  					replica information if it this specific version:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[bucket
	.async()
    .getFromReplica("id", ReplicaMode.ALL)
    .filter(document -> document.content().getInt("version") > 5)
    .first()
    .subscribe();]]></codeblock>
  		</sectiondiv>
  			
  		<sectiondiv id="read-and-lock">
  		<p><b>Reading and locking</b></p>
  				
  				<p>Reading and locking works very similar to a regular read, but in addition the
  					<codeph>Document</codeph> is <b>write locked</b> (not read locked) on the server side
  					for the given amount of time.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Get and lock for 10 seconds
JsonDocument doc = bucket.getAndLock("id", 10);]]></codeblock>
  				
  				<p>
  					<note>You can only write lock a document for a maximum of 30 seconds. If an invalid lock
  						time (less than 0 or greater than 30 seconds) is provided, 15 seconds is used as the
  						default.</note>
  				</p>
  				
  				<p>The <codeph>Document</codeph> is unlocked under the following conditions:</p>
  				
  				<ul>
  					<li>The <codeph>unlock()</codeph> command is used.</li>
  					<li>The <codeph>Document</codeph> is replaced with the correct CAS value.</li>
  					<li>30 seconds are over, and the server unlocks it for you.</li>
  				</ul>
  				<p>The following example shows the case with optimistic locking, where the locked document
  					is automatically released on error so it can be used by other clients.</p>
  				<codeblock outputclass="language-java"><![CDATA[
JsonDocument doc = JsonDocument.create(
	"stats",
	JsonObject.empty().put("sold", 0).put("bought", 0)
);
bucket.upsert(doc);

doc = bucket.getAndLock("stats", 20);
try {
    doc.content().put("sold", doc.content().getInt("sold") + 1);
    // Fake a processing error
    if (new Random().nextInt(100) < 30) {
        throw new RuntimeException("processing error");
    }
} catch (RuntimeException ex) {
    bucket.unlock("stats", doc.cas());
}
]]></codeblock>
  		</sectiondiv>
  			
  			<sectiondiv>
  				<p><b>Reading and touching</b></p>
  				
  				<p>Reading and touching works very similar to a regular read, but it also refreshes the
  					expiration time of the document to the specified value.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Get and set the new expiration time to 4 seconds
JsonDocument doc = bucket.getAndTouch("id", 4);]]></codeblock>
  				
  				<p>You can also use the <codeph>touch()</codeph> command if you do not want to read the
  					document and just refresh its expiration time.</p>
  				
  				<note type="important">If you specify an expiration time greater than 30 days in seconds (60
  					seconds * 60 minutes * 24 hours * 30 days = 2,592,000 seconds), it is considered an
  					absolute timestamp instead of a relative one.</note>
  				
  			</sectiondiv>
      
      
    </section>
  	
  	
  	
  	
  	
  	
  	
  	
  	<section><title>Deleting Documents</title>
  		
  		Describes how to delete documents using the <codeph>remove()</codeph> method.
  	
  			
  	<sectiondiv>
  				<p><b>Removing</b></p>
  				
  				<p>You can remove a document by utilizing the <codeph>remove()</codeph> method.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Remove the document by its ID.
JsonDocument doc = bucket.remove("id");]]></codeblock>
  				
  				<p>If you pass in a document which also has the CAS value populated, the SDK will make sure to only delete the document if they match:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Create document with some content
JsonDocument stored = bucket.upsert(JsonDocument.create("mydoc", JsonObject.create()));

// Delete it with the CAS check included
JsonDocument removed = bucket.remove(stored);]]></codeblock>
  				
  				<p>If successful, the returned <codeph>Document</codeph> has the <codeph>id</codeph> and <codeph>cas</codeph> fields populated,
  					all other fields are set to their default values.</p>
  				
  				<p>If the document is not found, a <codeph>DocumentNotFoundException</codeph> is raised.
  					When the document is found but the <codeph>cas</codeph> values do not match, a <codeph>CASMismatchException</codeph> is raised.</p>
  				
  	</sectiondiv>
  			
  	<sectiondiv>	
  				<p><b>Durability Requirements</b></p>
  				
  				<p>If no durability requirements are set on the <codeph>remove</codeph> method, the operation
  					will succeed when the server acknowledges the document delete in its managed cache
  					layer. While this is a performant operation, there might be situations where you
  					want to make sure that your document deletion has been persisted or replicated so
  					that it survives power outages and other node failures.</p>
  				
  				<p>The remove method provides overloads to supply such requirements:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument remove(String id, PersistTo persistTo);
JsonDocument remove(String id, ReplicateTo replicateTo);
JsonDocument remove(String id, PersistTo persistTo, ReplicateTo replicateTo);

D remove(D document, PersistTo persistTo);
D remove(D document, ReplicateTo replicateTo);
D remove(D document, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
  				
  				<p>You can configure either just one or both of the requirements when removing.
  					From an application point of view nothing needs to be changed when working with the response,
  					although there is something that need to be kept in mind:</p>
  				
  				<p>The internal implementation first performs a regular <codeph>remove</codeph> operation and
  					afterwards starts polling the specifically affected cluster nodes for the state of
  					the document. If something fails during this operation (and failing the
  					<codeph>Observable</codeph>), the original operation might have succeeded
  					nonetheless.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Remove the document and make sure the delete is persisted.
JsonDocument doc = bucket.remove("id", PersistTo.MASTER);

// Remove the document and make sure the delete is replicated.
JsonDocument doc = bucket.remove("id", ReplicateTo.ONE);

// Remove the document and make sure the delete is persisted and replicated.
JsonDocument doc = bucket.remove("id", PersistTo.MASTER, ReplicateTo.ONE);]]></codeblock>
  				
  	</sectiondiv>
  	</section>
  		
  		
  		<section><title>Atomic Operations</title>
  			<p>The <codeph>CouchbaseBucket</codeph> class provides atomic operations such as
  				<codeph>counter()</codeph>, <codeph>append()</codeph>, and <codeph>prepend()</codeph>.</p>
  			<sectiondiv>
  					<p>Counter</p>
  					
  					
  					<p>The <codeph>counter()</codeph> method allows you to increment or decrement a document with
  						a numerical content atomically. The method only accepts and returns a
  						<codeph>JsonLongDocument</codeph>. The value stored in the document is incremented or
  						decremented depending on the given <codeph>delta</codeph>: if the delta value is a positive
  						number, the value is incremented, and if it is a negative number, the value is decremented.
  						You can also pass in an initial value and an expiration time.</p>
  					
  					
  					<codeblock outputclass="language-java"><![CDATA[// Increase the counter by 5 and set the initial value to 0 if it does not exist
JsonLongDocument doc = bucket.counter("id", 5, 0);]]></codeblock>
  					
  					<p>The resulting document contains the new counter value. A very common use case is to implement
  						an increasing <codeph>AUTO_INCREMENT</codeph> like counter, where every new user just
  						gets a new ID (here using the asynchronous API):</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[bucket.async()
    .counter("user::id", 1, 1)
    .map(new Func1<JsonLongDocument, String>() {
        @Override
        public String call(JsonLongDocument counter) {
            return "user::" + counter.content();
        }
    })
    .flatMap(new Func1<String, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(String id) {
            return bucket.insert(JsonDocument.create(id, JsonObject.empty()));
        }
    }).subscribe();]]></codeblock>
  					
  					
  					<p>This code increases the counter by one, and then maps the returned number onto a custom
  						document ID (here the code prefixes <codeph>user::</codeph>). Afterward, the
  						<codeph>insert</codeph> method is executed with the generated ID and an empty document
  						content. Because a <codeph>counter</codeph> operation is atomic, the code is guaranteed to
  						deliver different user IDs, even when called at the same time from multiple threads.</p>
  					
  					<p>The counter always needs to be
  						greater than or equal to zero because negative values are not allowed. If you want to
  						decrement a counter, make sure to set it to a value greater than zero initially.</p>
  					
  					
  					
  					<p>If the initial value is omitted and the counter doesn't exists, this is signaled to the
  						user by propagating a <codeph>DocumentDoesNotExistException</codeph> (since 2.2.0). You can
  						avoid that by providing an explicit initial value, which could be the same as the delta or
  						even an arbitrary initial value (the delta won't be added to it at counter creation):</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[// Increase the counter by 5 or create the counter with a value of 4 if it does not exist
JsonLongDocument doc = bucket.counter("id", 5, 4);]]></codeblock>
  					
  					<p>If you want to set an expiration time, you need to provide both the initial value and the
  						expiration time. This constraint is imposed by the API because just exposing the expiration
  						time would be ambiguous with the initial value (<codeph>long</codeph> and
  						<codeph>int</codeph>).</p>
  					
  					
  					<codeblock outputclass="language-java"><![CDATA[// Increment by 5, initial 5 and 3 second expiration
JsonLongDocument doc = bucket.counter("id", 5, 5, 3);]]></codeblock>
  					
  			</sectiondiv>
  				
  		<sectiondiv>	
  				<p><b>Append &amp; Prepend</b></p>
  					
  					<p>Appending and prepending values to existing documents is also possible. Both the
  						<codeph>append</codeph> and <codeph>prepend</codeph> operation are atomic so that they can
  						be used without further synchronization.</p>
  					
  					
  					<p>
  						<note>Both operations only work on binary documents, ideally strings or byte arrays. It does not
  							work on JSON documents because it doesn't do any further inspection. Applying
  							one of those operations on a JSON document will render it invalid.</note>
  					</p>
  					
  					<p>A <codeph>Document</codeph> needs to be created before values can be appended or prepended.
  						Here is an example that creates a document and then appends a string to it:</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[bucket
    .insert(LegacyDocument.create("doc", "Hello, "))
    .flatMap(doc ->
        bucket.append(LegacyDocument.create("doc", "World!"))
    )
    .flatMap(bucket::get)
    .toBlocking()
    .forEach(doc -> System.err.println(doc.content()));]]></codeblock>
  					
  					<p>When executed, this code prints <codeph>Hello, World!</codeph>.</p>
  		</sectiondiv>
  				
  			<sectiondiv>	
  					<p><b>Durability Requirements</b></p>
  					
  					
  					<p>If no durability requirements are set on the <codeph>append</codeph>, <codeph>prepend</codeph>
  						or <codeph>counter</codeph> methods, the operation will succeed when the server
  						acknowledges the document in its managed cache layer. While this is a performant
  						operation, there might be situations where you want to make sure that your document
  						has been persisted or replicated so that it survives power outages and other node
  						failures.</p>
  					
  					
  					<p>All atomic operations provide overloads to supply such durability requirements:</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[D append(D document, PersistTo persistTo);
D append(D document, ReplicateTo replicateTo);
D append(D document, PersistTo persistTo, ReplicateTo replicateTo);

D prepend(D document, PersistTo persistTo);
D prepend(D document, ReplicateTo replicateTo);
D prepend(D document, PersistTo persistTo, ReplicateTo replicateTo);

JsonLongDocument counter(String id, long delta, PersistTo persistTo);
JsonLongDocument counter(String id, long delta, ReplicateTo replicateTo);
JsonLongDocument counter(String id, long delta, PersistTo persistTo, ReplicateTo replicateTo);

JsonLongDocument counter(String id, long delta, long initial, PersistTo persistTo);
JsonLongDocument counter(String id, long delta, long initial, ReplicateTo replicateTo);
JsonLongDocument counter(String id, long delta, long initial, PersistTo persistTo, ReplicateTo replicateTo);

JsonLongDocument counter(String id, long delta, long initial, int expiry, PersistTo persistTo);
JsonLongDocument counter(String id, long delta, long initial, int expiry, ReplicateTo replicateTo);
JsonLongDocument counter(String id, long delta, long initial, int expiry, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
  					
  					<p>You can configure either just one or both of the requirements. From an application point of view nothing needs to be
  						changed when working with the response, although there is something that need to be kept in mind:</p>
  					
  					<p>The internal implementation first performs a regular operation and afterward starts polling
  						the specifically affected cluster nodes for the state of the document. If something
  						fails during this operation (and failing the <codeph>observable</codeph>), the
  						original operation might have succeeded nonetheless.</p>
  					
  			</sectiondiv>
  			
  			
  		</section>		
  		


  	
  	<section><title>Bulk Operations</title>
  	
  	Bulk operations allow you to operate on more than one document at the same
  			time.
  	
  			
  			<sectiondiv>
  			<p><b>Introduction</b></p>
  				<p>To get better resource utilization, you need to perform all types of operations in
  					batches. Because of the asynchronous nature of the underlying core package, you can
  					utilize RxJava's operations to provide implicit batching facilities combined with the
  					asynchronous operations of the SDK.</p>
  				
  				<p>If you understand the general approach to batching, you can apply it to any operation against
  					the SDK, not just with <codeph>get()</codeph> calls like in the 1.x series SDK.</p>
  			</sectiondiv>
  			
  			<sectiondiv>
  				<p><b>Batching with RxJava</b></p>
  				
  				
  				<p>Implicit batching is performed by utilizing a few operators: <ul>
  					<li><codeph>Observable.just()</codeph> or <codeph>Observable.from()</codeph> to
  						generate an <codeph>observable</codeph> that contains the data you want to batch
  						on.</li>
  					<li><codeph>flatMap()</codeph> to send those events against the Couchbase Java SDK
  						and merge the results asynchronously.</li>
  					<li><codeph>last()</codeph> if you want to wait until the last element of the batch
  						is received.</li>
  					<li><codeph>toList()</codeph> if you care about the responses and want to aggregate
  						them easily.</li>
  					<li>If you have more than one subscriber, using <codeph>cache()</codeph> to prevent
  						accessing the network over and over again with every subscribe.</li>
  				</ul></p>
  				
  				<p>The following example creates an observable stream of 5 keys to load in a batch,
  					asynchronously fires off <codeph>get()</codeph> requests against the SDK, waits until
  					the last result has arrived, and then converts the result into a list and blocks at the
  					very end:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[Cluster cluster = CouchbaseCluster.create();
Bucket bucket = cluster.openBucket();

List<JsonDocument> foundDocs = Observable
    .just("key1", "key2", "key3", "key4", "key5")
    .flatMap(new Func1<String, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(String id) {
            return bucket.async().get(id);
        }
    })
    .toList()
    .toBlocking()
    .single();]]></codeblock>
  				
  				<p>Note that this always returns a list, but it may contain 0 to 5 documents, depending on how
  					many are actually found. Also, at the very end the observable is converted into a
  					blocking one, but everything before that, including the network calls and the
  					aggregation, is happening completely asynchronously.</p>
  				
  				<p>Inside the SDK, this provides much more efficient resource utilization because the requests
  					are very quickly stored in the internal <codeph>Request RingBuffer</codeph> and the I/O
  					threads are able to pick batches as large as they can. Afterward, whatever server
  					returns a result first it is stored in the list, so there is no serialization of
  					responses going on.</p>
  				
  				<p>If you wrap the code in a helper method, you can provide very nice encapsulated batching semantics:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[public List<JsonDocument> bulkGet(final Collection<String> ids) {
    return Observable
        .from(ids)
        .flatMap(new Func1<String, Observable<JsonDocument>>() {
            @Override
            public Observable<JsonDocument> call(String id) {
                return bucket.async().get(id);
            }
        })
        .toList()
        .toBlocking()
        .single();
}]]></codeblock>
  				
  			</sectiondiv>
  			
  	<sectiondiv>		
  				<p><b>Batching mutations</b></p>
  				
  				<p>The previous Java SDK only provided bulk operations for <codeph>get()</codeph>. With the
  					techniques shown above, you can perform any kind of operation as a batch operation.</p>
  				
  				<p>The following code generates a number of fake documents and inserts them in one batch. Note that you can decide to either collect the results with <codeph>toList()</codeph> as shown above or just use <codeph>last()</codeph> as shown here to wait until the last document is properly inserted.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Generate a number of dummy JSON documents
int docsToCreate = 100;
List<JsonDocument> documents = new ArrayList<JsonDocument>();
for (int i = 0; i < docsToCreate; i++) {
    JsonObject content = JsonObject.create()
        .put("counter", i)
        .put("name", "Foo Bar");
    documents.add(JsonDocument.create("doc-"+i, content));
}

// Insert them in one batch, waiting until the last one is done.
Observable
    .from(documents)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(final JsonDocument docToInsert) {
            return bucket.async().insert(docToInsert);
        }
    })
    .last()
    .toBlocking()
    .single();]]></codeblock>
  				
  	</sectiondiv>
  			
  			<sectiondiv>
  			<p><b>Performance</b></p>
  				
  				<p>Here are two code samples, both synchronous, that showcase serialized and batched loading of
  					documents. Note that more important than the absolute operations per second is the
  					relative improvement with the same workload.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Serialized workload of loading documents
while(true) {
    List<JsonDocument> loaded = new ArrayList<JsonDocument>();
    int docsToLoad = 10;
    for (int i = 0; i < docsToLoad; i++) {
        JsonDocument doc = bucket.get("doc-" + i);
        if (doc != null) {
            loaded.add(doc);
        }
    }
}]]></codeblock>
  				
  				<p>
					<image href="images/batching-single.png" id="image_uht_xb3_yv"/></p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Same workload, utilizing batching effects
while(true) {
    int docsToLoad = 10;
    Observable
        .range(0, docsToLoad)
        .flatMap(new Func1<Integer, Observable<JsonDocument>>() {
            @Override
            public Observable<JsonDocument> call(Integer i) {
                return bucket.async().get("doc-"+i);
            }
        })
        .toList()
        .toBlocking()
        .single();
        
}]]></codeblock>
  				
  				<p>
  					<image href="images/batching-bulk.png" id="image_batch_bulk"/>
  				</p>
  				
  				
  			</sectiondiv>
  			
  	<sectiondiv>
  			<p><b>Error Handling &amp; Recovery</b></p>
  				
  				<p>Technically speaking, error handling in bulk operations is similar to generic
  					<codeph>Observable</codeph> error handling, but because the topic is strongly related
  					the most important concepts are covered here as well.</p>
  				
  				<p>In general, the following questions come up:</p>
  				
  				<ul>
  					<li>How can I implement best effort loading and just return the values that were
  						successful?</li>
  					<li>What are <codeph>BackpressureExceptions</codeph> and how can I handle them?</li>
  					<li>How can I retry individual operations in the batch when they fail?</li>
  				</ul>
  				
  				<p>When handling these situations, an important fact to remember is that as soon as an
  					error happens inside an <codeph>observable</codeph>, the whole thing is terminated. If
  					you want the whole stream to complete, error handling needs to be as close as possible
  					to the original source. Let's take the bulk loading of documents as an example which we
  					are going to modify to be more resilient:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id);
    })
    .subscribe();]]></codeblock>
  				
  				<p>To implement the best effort use case, you can ignore all errors on each
  					<codeph>get()</codeph> result before it gets merged and flattened into the original
  					stream. It is strongly recommended to log the error, because otherwise you'll never know
  					what went wrong in the first place for each failing operation.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id)
            .doOnError(System.err::println) // print the error, log,...
            .onErrorResumeNext(Observable.empty()); // on error resume with an empty sequence
    })
    .subscribe();]]></codeblock>
  				
  				<p>There is a slight variation to that which you can use instead. RxJava provides a <xref href="http://reactivex.io/documentation/operators/merge.html" format="html" scope="external">mergeDelayError</xref> operator that merges individual observables, emits all items and then at the very end fails the observable with a <codeph>CompositeException</codeph>. This composite exception contains all errors that have happened so you can do something with them at a later point.</p>
  				
  				<p>Very often you want a complete result and therefore you need to retry individual
  					operations if an error happened. It is recommended to retry based on a defined strategy
  					for specific exception types and propagate the error for unknown exceptions or those
  					types which are known to be permanent. For a full list of errors that can happen and
  					their implications, see the Javadoc API reference for the <codeph>Bucket</codeph>
  					methods you are using.</p>
  				
  				<p>Since the <codeph>BackpressureException</codeph> has been frequently referenced in the past, we are going to use that one as an example. The same logic of course applies to all other types as well.</p>
  				
  				<p>The <codeph>BackpressureException</codeph> is used to shed load on the request side
  					and fails your Observable quickly if the underlying system is in an overload condition.
  					The reason for this is that somehow requests are produced more quickly than responses
  					can be generated (because that includes the actual network round trip). This is common
  					in bulk scenarios since it could be that you are requesting a very large set of
  					documents at the same time which puts temporary pressure on the client.</p>
  				
  				<p>To solve that, we can apply a delayed retry algorithm onto the Observable so it is retried at a later point. We are making use of the <codeph>Delay</codeph> construct shipped with the 2.1 SDK which provides a very convenient way to generated increasing delays. You also want to stop retrying at some point so the operation is not retried forever.</p>
  				
  				<p>Since 2.1.2, the <codeph>RetryBuilder</codeph> API has been introduced to help you build retry scenarios. The following code retries with an exponential backoff (with a 100 millisecond
  					ceiling), but stops after 10 attempts and propagates the error.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[
Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id)
            .retryWhen(RetryBuilder
                .anyOf(BackpressureException.class)
                .delay(Delay.exponential(TimeUnit.MILLISECONDS, 100))
                .max(10)
                .build()
            );
    })
    .subscribe();]]></codeblock>
  				
  				<p>For reference, this is how you would have written the retry feature prior to 2.1.2:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[final Delay delay = Delay.exponential(TimeUnit.MILLISECONDS, 100);
  					
Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id)
            .retryWhen(notification ->
                notification
                    .zipWith(Observable.range(1, 11), Tuple::create)
                    .flatMap(att ->
                        att.value2() == 9 || !(att.value1() instanceof BackpressureException)
                            ? Observable.error(att.value1())
                            : Observable.timer(delay.calculate(att.value2()), delay.unit())
                    )
            );
    })
    .subscribe();]]></codeblock>
  				
  				<p>This code zips the error with a range that indicates the number of attempts we want
  					to retry. If this is over 10 attempts or the error is not a backpressure exception, the
  					error will be propagated.</p>
  				
  				<p>Finally, you always want to chain in <codeph>timeout()</codeph> calls so you have a
  					last resort error and you can be sure that the code you're relying on isn't stuck
  					forever. You can also use methods like <codeph>onErrorReturn()</codeph> to return a
  					stubbed object or a fixed entity that you know will never fail (so in the worst case you
  					can provide a reduced user experience instead of failing completely).</p>
  	</sectiondiv>
  			</section>
  		
  		

  
  </body>
</topic>
