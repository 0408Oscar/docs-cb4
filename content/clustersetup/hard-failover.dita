<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_a4s_d24_vs">
  <title>Using Hard Failover</title><shortdesc>Hard failover is the process to eject immediately an unavailable or unstable node from a
    cluster. </shortdesc>
  <body>
    <note type="note">To fully understand how Hard Failover works, you must first understand how
      Couchbase partitions data across a cluster. If you are not familiar, click <xref
        href="../concepts/buckets-vbuckets.dita#concept_v22_mvy_zs">here</xref>.</note>
    <p>Hard Failover is the act of ejecting a node from a cluster that is unavailable or unstable
      (for example, server/instance failure). Initiating a Hard Failover tells the Cluster Manager
      to remove the node from active use, regardless of what is occurring, and then initiate the
      procedures necessary for the other nodes in the cluster to take over for the ejected node. The
      Cluster Manager performs slightly different actions for each service (Data, Index, Query, Full
      Text Search) running on the node being failed over. We will discuss these service-specific
      actions later in this section. </p>
    <p>You can perform hard failover on any node in the cluster.</p> 
    <note type="note">Do not use hard failover for regular planned maintenance. For that purpose see
      the explanations in <xref href="setup-failover-graceful.dita#topic_ysk_ycm_zs"/> or <xref
        href="remove-nodes.dita#topic_byh_lbg_q4"/>.</note>
    
    <p>Hard failover can be initiated in multiple ways:</p>
    <ul>
      <li>Manually via the Couchbase Web Console</li>
      <li>Using the CLI or RESTful API from a <filepath>script/program/ops</filepath> platform</li>
      <li>Using the Automatic Failover functionality performed by the Couchbase cluster manager</li>
    </ul>
    
    <p>Hard failover is the type of failover that automatic failover triggers when the Cluster
      Manager determines that a node is unavailable and eligible for automatic failover. To read
      more about this, see <xref href="automatic-failover.dita#topic_fcf_chm_zs"/>.</p>
    
    <section><title>How Hard Failover Effects Nodes in the Cluster</title>
      <p>Hard failover affects each of the services slightly different, be they on the same
        server/instance or using  <xref
          href="../concepts/multidimensional-scaling.dita#concept_ckt_svy_zs"/>and on different
        servers/instances.</p>
     
   </section> 
    <section><title>The Data Service</title>
      <p>If the node being failed over is running the Data service, the cluster initiates multiple
        operations to make an active set of vBuckets available as soon as possible. Perform a hard
        failover only when there is a full compliment of replica vBuckets available to promote for
        each Bucket. </p>
      <p>If you have four nodes running the Data service, and one node has failed, there are 256
        active and 256 replica vBuckets missing from the cluster. The vBucket resources will look
        like below: </p>
      <p><image href="../admin/picts/vbucket-resources.png" id="image_c2r_ycf_5v" align="left"
          width="350"/></p>
      <p>The moment a Hard Failover is initiated, (either by the web UI, CLI, REST API or automatic
        failover) the Couchbase Cluster Manager communicates with the remaining nodes in the Data
        service to begin the process. The Cluster Manager knows where all active and replica
        vBuckets live across the cluster, as well as the physical topology of the cluster. It knows
        which vBuckets were active on the failed node and which of the still existing nodes in the
        cluster house their corresponding replica vBuckets.</p>
     <ul>
       <li>The Cluster Manager directs the remaining nodes to promote the replica vBuckets to active
          status on those nodes. There is no movement or copying of data. The replicas already exist
          on the remaining nodes; it is simply a matter of flipping their status from replica to
          active. Because of this, the promotion is nearly instantaneous on the vBuckets in
          question. </li>
       <li>Since this is a cluster topology change, the Cluster Manager communicates the new cluster map to the client SDKs, 
         other clusters replicating to it via XDCR, etc.</li>
     </ul>
      <p>The entire process from initiating the node failover to completion is usually in the single digit milliseconds timeframe. 
        The timing will vary slightly depending on how many buckets and vBuckets were on the failed node.</p>
      <p>When this process is complete, there will be no vBuckets left on the failed-over node, but
        the node has not been fully ejected from the cluster. The cluster is considered to be in a
        degraded state, both regarding the missing node and High Availability; as the corresponding
        replica vBuckets on the remaining nodes of the cluster have been sacrificed to get 100% of
        the data running. Unless there were already additional replica data sets for each bucket,
        the cluster could not withstand another node going down. The cluster does not
        auto-regenerate missing vBuckets, and once that happens it narrows your recovery options.
        The cluster also does not want to introduce a cascading failure and potentially make a minor
        problem more severe.</p>
      <p>Because of this, the cluster needs to be returned to a stable state as soon as possible.</p>
   </section> 
    <section><title>The Index Service</title>
      <p>When Global Secondary Indexes (GSI) are defined, each is created on only one node running
        the Index Service unless it is specified otherwise. If that node fails, those indexes and
        their definitions on that node are unavailable. Should the node be repaired and added back
        in via Delta Node Recovery, the indexes on that node will be updated and become available
        again. If the node is irreparable and replaced, the indexes will need to be created again.
        To rebalance a new node into the cluster, you must first eject the failed node, using hard
        failover.</p>
       
      <p>To mitigate the indexes being unavailable, it is the best practice that secondary indexes
        are created on at least two nodes of the Index service. For more information, go to the
        documentation on <xref href="../indexes/indexing-overview.dita#concept_ssb_qhb_ys"/>.</p>
       
       
     </section> 
    <section><title>The Query Service</title>
   
      <p>For the Query service, the nodes are stateless and can be added and removed from the
        cluster with no consequence to data. However, any in-flight queries on those node(s) will
        error. As long as the node(s) to be removed are not the last Query service node in the
        cluster, the cluster will continue to function. As one might expect when node capacity is
        reduced, the cluster can experience higher load on the remaining node(s) which can lead to
        performance issues on queries.</p>
   
 </section>
    <section><title>The Full Text Search (FTS) Service</title>
      <p>Full text indexes are partitioned and automatically distributed across nodes if multiple
        nodes are running the FTS service. When a node running the FTS service is failed over, it
        stops taking traffic. If there are no other nodes running the FTS Service, all full text
        index building stops and full text queries fail. If there is at least one other node running
        FTS Service, other nodes continue responding to queries and return partial results, which
        your application may choose to accept or not, depending on your requirements. </p>   
      <p>When the administrator rebalances the cluster,  the cluster performs multiple operations depending on the level of 
        redundancy you have configured for the FTS service.</p>
      <ul>
        <li>If replicas are configured for a full text index, the FTS Service promotes the replicas to active on the remaining nodes of the cluster.</li>
        <li>If replicas are not configured for a full text index, the FTS Service rebuilds the indexes on 
          the remaining nodes of the cluster using stored index definitions.</li>
      </ul>
      <p>FTS Service does not perform delta node recovery.</p>
      
    </section>
    <section><title>Returning the Cluster to a Stable State</title>
     
      <p>If/When the failed node is repaired and ready to be added back to the cluster, it can be
        added back via <xref href="delta-node-recovery.dita#topic_gsv_g24_vs">Delta recovery</xref>,
          <xref href="full-recovery.dita#topic_v5t_324_vs">Full recovery</xref> or an entirely new
        node could be added.</p>
      <ul>
        <li>If Delta node recovery is an option, the Cluster Manager recognizes this node as a
          previous member of the cluster. If using the Couchbase Web Console, it will prompt the
          administrator to perform the delta node recovery. In the CLI, it will either perform the
          delta node recovery or it will fail and inform that you have to perform a full recovery.
            <p>When a node is added back to the cluster using delta node recovery, the replica
            vBuckets on the failed-over node are considered trusted, but behind on data. The Cluster
            Manager will coordinate the vBuckets to become resynchronized. This catches up the
            vBuckets on the node from where they left off to be current. When the Cluster Manager
            has finished the synchronization, the vBucket is promoted back to active status, and the
            cluster map is updated since this is a topology change.</p></li>
        <li>If the node is added back using Full Recovery, it is treated as an entirely new node
          added to the cluster:  it will be reloaded with data, and needs a rebalance.</li>
        <li>The other option is through <xref href="adding-nodes.dita#topic_ncd_gdg_q4"/>and <xref
            href="rebalance.dita#topic_xsx_1mn_vs"/>.</li>
      </ul>
      <p>If you can, always try to attempt on returning the cluster to a properly sized topology
        before rebalancing. If you do a rebalance before adding the node back in, you can no longer
        perform a delta node rebalancing.</p>    
   </section> 
    <section><title>A Hypothetical Scenario</title>
      <p>Imagine a Couchbase bucket distributed across four nodes of the Data service in a cluster,
        where a node needs to be removed right this moment. The server operations on-call engineer
        calls you at 11 PM on a Friday night to say that node #4 of the cluster is down. The ops
        team has been unable to get the server back up for the last 10 minutes.</p>
      <p>You have followed best practices and have auto-failover configured. For the Data service,
        with a four node cluster and one replica for each Bucket, there are 256 active and 256
        replica vBuckets on each of the four nodes, totaling 1024 active and 1024 replica vBuckets.
        This particular example will only talk about one vBucket, #762, but this process is repeated
        for all the vBuckets on the node to be failed over:</p>  
    <ol>
      <li>A hard failover is initiated (automatically or manually) to remove the node where active vBucket 762 resides, node 4 in this example.</li>
      <li>The cluster manager promotes replica vBucket 762 to active status on node 2. 
        Note, this leaves the cluster with no replica for vBucket 762 until a 
        rebalance or a delta node recovery, unless there are more replicas configured for this bucket.</li>
      <li>As this is a cluster topology change, the cluster map is updated so subsequent reads and writes by the Couchbase client SDKs 
        will go to the correct location for data in vBucket 762, now node 2.</li>
    </ol>
      <p>This process all happens in fractions of a second. The process is then repeated for the remaining 255 vBuckets of the bucket, one bucket at a time. 
        If there were more buckets, it would proceed to the next bucket and repeat the process there until complete.</p>  
      <p>What is happening in the application during this process, one may ask? Until the down node
        is failed over (either automatically or manually) to promote the replica vBuckets to active,
        the application is receiving errors or timeouts for one-quarter of the reads and writes that
        would have gone to the now down node. We had four nodes; now we have three. If there were
        ten nodes in the Data service, the application would be unable to address one tenth of the
        data until failover is initiated. If the application needs to read before failover happens,
        the application developer may want to use Replica Reads (see SDK-specific documentation),
        which is only for a circumstance like this.</p>
  </section>  
    <section><title>Why Might One Use Hard Failover instead of Graceful Failover?</title>
      <p>Hard failover is meant to be performed as a reactive action to an unhealthy node in the
        cluster. Graceful failover is meant for planned maintenance. Use hard failover when an
        unhealthy node needs to be ejected from the cluster right now and get back to 100% of the
        data available as soon as possible.</p>
  <dl>
    <dlentry>
      <dt>Hard Failover and multiple nodes</dt>
      <dd>You should only failover multiple nodes at a time if there are enough replicas across all
            buckets of the Data service, and there are enough servers left so that the cluster can
            continue to operate.<p>Normally one would be able to fail over one node per replica
              configured in the bucket/cluster. For example, if you require the ability to fail over
              two nodes, you must configure two replicas for each bucket. Failure to do so will
              result in data loss. Simply put, do not failover more nodes than there are replicas
              configured for all buckets.</p><p>The exception to the above rule is when the
              Rack/Zone Awareness (RZA) feature is configured. RZA allows designating which nodes
              are in a server rack in a data center, different VM hosts or availability zones in a
              cloud hosting provider. It ensures that the replica vBuckets for the nodes in Rack A
              are never in Rack A. When using RZA, it is safe to failover an entire rack’s worth of
              Couchbase nodes without data loss or interrupting your application: because the other
              racks contain nodes with the replicas. For more information see <xref
                href="../architecture/smart-data-placement-rack-zone-aware.dita#concept_bln_cnv_vs"
              />. </p></dd>
    </dlentry>
  </dl>
  <dl>
    <dlentry>
      <dt>Hard Failover when the cluster has not recognized that the node is down</dt>
      <dd>In rare cases, Cluster Manager might fail to recognize that an unhealthy node is down. If
            this occurs and a graceful failover is not successful, a hard failover can be the
            answer. To initiate a hard failover for a node in this state, select the “Fail Over”
            button in the Web UI or use the command line. If the node’s health issue can be
            resolved, the node may be added back to the cluster. A delta recovery will be presented
            as an option if the Cluster Manager detects that it is possible. Otherwise a full
            recovery must be used. If the issue cannot be resolved, a replacement node should be
            added and then the cluster rebalanced. It is important to always restore the cluster to
            a properly sized topology before rebalancing. Otherwise, you might cause additional
            failures as nodes become overloaded.</dd>
    </dlentry>
  </dl>
  
</section>    
  </body>
</topic>
